{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from easydict import EasyDict as edict\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.parallel\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy \n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "__C = edict()\n",
    "cfg = __C\n",
    "\n",
    "# Dataset name: flowers, birds\n",
    "__C.DATASET_NAME = 'birds'\n",
    "__C.CONFIG_NAME = ''\n",
    "__C.DATA_DIR = '../data/birds/'\n",
    "__C.MODEL_PATH = './Model/'\n",
    "__C.GPU_ID = 0\n",
    "__C.CUDA = False\n",
    "__C.WORKERS = 6\n",
    "__C.N_WORDS = 5450\n",
    "\n",
    "__C.RNN_TYPE = 'LSTM'   # 'GRU'\n",
    "__C.B_VALIDATION = False\n",
    "\n",
    "__C.TREE = edict()\n",
    "__C.TREE.BRANCH_NUM = 3\n",
    "__C.TREE.BASE_SIZE = 64\n",
    "\n",
    "\n",
    "# Training options\n",
    "__C.TRAIN = edict()\n",
    "__C.TRAIN.BATCH_SIZE = 2\n",
    "__C.TRAIN.MAX_EPOCH = 600\n",
    "__C.TRAIN.SNAPSHOT_INTERVAL = 2000\n",
    "__C.TRAIN.DISCRIMINATOR_LR = 2e-4\n",
    "__C.TRAIN.GENERATOR_LR = 2e-4\n",
    "__C.TRAIN.ENCODER_LR = 2e-4\n",
    "__C.TRAIN.RNN_GRAD_CLIP = 0.25\n",
    "__C.TRAIN.FLAG = True\n",
    "__C.TRAIN.NET_E = '../DAMSMencoders/bird/text_encoder200.pth'\n",
    "__C.TRAIN.NET_G = ''\n",
    "__C.TRAIN.B_NET_D = True\n",
    "\n",
    "__C.TRAIN.SMOOTH = edict()\n",
    "__C.TRAIN.SMOOTH.GAMMA1 = 5.0\n",
    "__C.TRAIN.SMOOTH.GAMMA3 = 10.0\n",
    "__C.TRAIN.SMOOTH.GAMMA2 = 5.0\n",
    "__C.TRAIN.SMOOTH.LAMBDA = 1.0\n",
    "\n",
    "\n",
    "# Modal options\n",
    "__C.GAN = edict()\n",
    "__C.GAN.DF_DIM = 64\n",
    "__C.GAN.GF_DIM = 128\n",
    "__C.GAN.Z_DIM = 100\n",
    "__C.GAN.CONDITION_DIM = 100\n",
    "__C.GAN.R_NUM = 2\n",
    "__C.GAN.B_ATTENTION = True\n",
    "__C.GAN.B_DCGAN = False\n",
    "\n",
    "\n",
    "__C.TEXT = edict()\n",
    "__C.TEXT.CAPTIONS_PER_IMAGE = 10\n",
    "__C.TEXT.EMBEDDING_DIM = 256\n",
    "__C.TEXT.WORDS_NUM = 18\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    imgs, captions, captions_lens, class_ids, keys = data\n",
    "    # sort data by the length in a decreasing order\n",
    "    sorted_cap_lens, sorted_cap_indices = \\\n",
    "        torch.sort(captions_lens, 0, True)\n",
    "\n",
    "    real_imgs = []\n",
    "    for i in range(len(imgs)):\n",
    "        imgs[i] = imgs[i][sorted_cap_indices]\n",
    "        if cfg.CUDA:\n",
    "            real_imgs.append((imgs[i]).cuda())\n",
    "        else:\n",
    "            real_imgs.append((imgs[i]))\n",
    "\n",
    "    captions = captions[sorted_cap_indices].squeeze()\n",
    "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
    "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
    "    if cfg.CUDA:\n",
    "        captions = captions.cuda()\n",
    "        sorted_cap_lens = sorted_cap_lens.cuda()\n",
    "\n",
    "    return [real_imgs, captions, sorted_cap_lens,\n",
    "            class_ids, keys]\n",
    "\n",
    "\n",
    "def get_imgs(img_path, imsize, bbox=None,\n",
    "             transform=None, normalize=None):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "    if bbox is not None:\n",
    "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "        y1 = np.maximum(0, center_y - r)\n",
    "        y2 = np.minimum(height, center_y + r)\n",
    "        x1 = np.maximum(0, center_x - r)\n",
    "        x2 = np.minimum(width, center_x + r)\n",
    "        img = img.crop([x1, y1, x2, y2])\n",
    "\n",
    "    if transform is not None:\n",
    "        img = transform(img)\n",
    "\n",
    "    ret = []\n",
    "    if cfg.GAN.B_DCGAN:\n",
    "        ret = [normalize(img)]\n",
    "    else:\n",
    "        for i in range(cfg.TREE.BRANCH_NUM):\n",
    "            # print(imsize[i])\n",
    "            if i < (cfg.TREE.BRANCH_NUM - 1):\n",
    "                re_img = transforms.Resize(imsize[i])(img)\n",
    "            else:\n",
    "                re_img = img\n",
    "            ret.append(normalize(re_img))\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class TextDataset(Data.Dataset):\n",
    "    def __init__(self, data_dir, split='train',base_size=64,\n",
    "                 transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])    \n",
    "        self.target_transform = target_transform     \n",
    "        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE  # 10\n",
    "        self.imsize = []\n",
    "        for i in range(cfg.TREE.BRANCH_NUM):  # create a list of image sizes\n",
    "            self.imsize.append(base_size)\n",
    "            base_size = base_size * 2\n",
    "        self.data_dir = data_dir    \n",
    "        \n",
    "        # Load everything \n",
    "        \n",
    "        all_path = 'birds_everything.pickle'\n",
    "        with open(all_path, 'rb') as f:\n",
    "            self.filenames, self.bbox, self.captions, self.class_ids, self.ixtoword, self.wordtoix, self.n_words= pickle.load(f)\n",
    "            print('Load from:',all_path)\n",
    "            \n",
    "        # Define training or test:\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.filenames = self.filenames[:-3000]\n",
    "            self.captions = self.captions[:-3000]\n",
    "            self.class_ids = self.class_ids[:-3000]\n",
    "        else: \n",
    "            self.filenames = self.filenames[-3000:]\n",
    "            self.captions = self.captions[-3000:]\n",
    "            self.class_ids = self.class_ids[-3000:]\n",
    "            \n",
    "        self.number_example = len(self.filenames)\n",
    "        \n",
    "\n",
    "    def get_caption(self, sent_ix):\n",
    "        # a list of indices for a sentence\n",
    "        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n",
    "        if (sent_caption == 0).sum() > 0:\n",
    "            print('ERROR: do not need END (0) token', sent_caption)\n",
    "        num_words = len(sent_caption)\n",
    "        # pad with 0s (i.e., '<end>')\n",
    "        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n",
    "        x_len = num_words\n",
    "        if num_words <= cfg.TEXT.WORDS_NUM:\n",
    "            x[:num_words, 0] = sent_caption\n",
    "        else:\n",
    "            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n",
    "            np.random.shuffle(ix)\n",
    "            ix = ix[:cfg.TEXT.WORDS_NUM]\n",
    "            ix = np.sort(ix)\n",
    "            x[:, 0] = sent_caption[ix]\n",
    "            x_len = cfg.TEXT.WORDS_NUM\n",
    "        return x, x_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #\n",
    "        key = self.filenames[index]\n",
    "        cls_id = self.class_ids[index]\n",
    "        #\n",
    "        if self.bbox is not None:\n",
    "            bbox = self.bbox[key]\n",
    "            data_dir = '%s/CUB_200_2011' % self.data_dir\n",
    "        else:\n",
    "            bbox = None\n",
    "            data_dir = self.data_dir\n",
    "        #\n",
    "        img_name = '%s/images/%s.jpg' % (data_dir, key)\n",
    "        imgs = get_imgs(img_name, self.imsize,\n",
    "                        bbox, self.transform, normalize=self.norm)\n",
    "        # random select a sentence\n",
    "        sent_ix = np.random.randint(0, self.embeddings_num)\n",
    "        new_sent_ix = index * self.embeddings_num + sent_ix\n",
    "        caps, cap_len = self.get_caption(new_sent_ix)\n",
    "        return imgs, caps, cap_len, cls_id, key\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global attention takes a matrix and a query metrix.\n",
    "Based on each query vector q, it computes a parameterized convex combination of the matrix\n",
    "based.\n",
    "H_1 H_2 H_3 ... H_n\n",
    "  q   q   q       q\n",
    "    |  |   |       |\n",
    "      \\ |   |      /\n",
    "              .....\n",
    "          \\   |  /\n",
    "                  a\n",
    "Constructs a unit mapping.\n",
    "$$(H_1 + H_n, q) => (a)$$\n",
    "Where H is of `batch x n x dim` and q is of `batch x dim`.\n",
    "\n",
    "References:\n",
    "https://github.com/OpenNMT/OpenNMT-py/tree/fc23dfef1ba2f258858b2765d24565266526dc76/onmt/modules\n",
    "http://www.aclweb.org/anthology/D15-1166\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes):\n",
    "    \"1x1 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                     padding=0, bias=False)\n",
    "\n",
    "\n",
    "def func_attention(query, context, gamma1):\n",
    "    \"\"\"\n",
    "    query: batch x ndf x queryL # word_embe_dim = \n",
    "    ndf\n",
    "    context: batch x ndf x ih x iw (sourceL=ihxiw) #image\n",
    "    mask: batch_size x sourceL\n",
    "    \"\"\"\n",
    "    batch_size, queryL = query.size(0), query.size(2)\n",
    "    ih, iw = context.size(2), context.size(3)\n",
    "    sourceL = ih * iw\n",
    "\n",
    "    # --> batch x sourceL x ndf\n",
    "    context = context.view(batch_size, -1, sourceL) # -1 = ndf\n",
    "    contextT = torch.transpose(context, 1, 2).contiguous() \n",
    "\n",
    "    # Get attention\n",
    "    # (batch x sourceL x ndf)(batch x ndf x queryL)\n",
    "    # -->batch x sourceL x queryL\n",
    "    attn = torch.bmm(contextT, query) # Eq. (7) in AttnGAN paper\n",
    "    # --> batch*sourceL x queryL\n",
    "    attn = attn.view(batch_size*sourceL, queryL)\n",
    "    attn = nn.Softmax()(attn)  # Eq. (8)\n",
    "\n",
    "    # --> batch x sourceL x queryL\n",
    "    attn = attn.view(batch_size, sourceL, queryL)\n",
    "    # --> batch*queryL x sourceL\n",
    "    attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "    attn = attn.view(batch_size*queryL, sourceL)\n",
    "    #  Eq. (9)\n",
    "    attn = attn * gamma1\n",
    "    attn = nn.Softmax()(attn)\n",
    "    attn = attn.view(batch_size, queryL, sourceL)\n",
    "    # --> batch x sourceL x queryL\n",
    "    attnT = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "    # (batch x ndf x sourceL)(batch x sourceL x queryL)\n",
    "    # --> batch x ndf x queryL\n",
    "    weightedContext = torch.bmm(context, attnT)\n",
    "\n",
    "    return weightedContext, attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "\n",
    "class GlobalAttentionGeneral(nn.Module):\n",
    "    def __init__(self, idf, cdf):\n",
    "        \n",
    "        super(GlobalAttentionGeneral, self).__init__()\n",
    "        self.conv_context = conv1x1(cdf, idf)\n",
    "        self.sm = nn.Softmax()\n",
    "        self.mask = None\n",
    "\n",
    "    def applyMask(self, mask):\n",
    "        self.mask = mask  # batch x sourceL\n",
    "\n",
    "    def forward(self, input, context):  #{input:h_code, context:word_emb}\n",
    "        \"\"\"\n",
    "            input: batch x idf x ih x iw (queryL=ihxiw) # idf is the depth\n",
    "            context: batch x cdf x sourceL\n",
    "        \"\"\"\n",
    "        ih, iw = input.size(2), input.size(3)\n",
    "        queryL = ih * iw\n",
    "        batch_size, sourceL = context.size(0), context.size(2)\n",
    "\n",
    "        # --> batch x queryL x idf\n",
    "        target = input.view(batch_size, -1, queryL)\n",
    "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
    "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
    "        sourceT = context.unsqueeze(3)\n",
    "        # --> batch x idf x sourceL\n",
    "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
    "\n",
    "        # Get attention\n",
    "        # (batch x queryL x idf)(batch x idf x sourceL)\n",
    "        # -->batch x queryL x sourceL\n",
    "        attn = torch.bmm(targetT, sourceT)\n",
    "        # --> batch*queryL x sourceL\n",
    "        attn = attn.view(batch_size*queryL, sourceL)\n",
    "        if self.mask is not None:\n",
    "            # batch_size x sourceL --> batch_size*queryL x sourceL\n",
    "            mask = self.mask.repeat(queryL, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "        attn = self.sm(attn)  # Eq. (2)\n",
    "        # --> batch x queryL x sourceL\n",
    "        attn = attn.view(batch_size, queryL, sourceL)\n",
    "        # --> batch x sourceL x queryL\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # (batch x idf x sourceL)(batch x sourceL x queryL)\n",
    "        # --> batch x idf x queryL\n",
    "        weightedContext = torch.bmm(sourceT, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        return weightedContext, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        nc = x.size(1)\n",
    "        assert nc % 2 == 0, 'channels dont divide 2!'\n",
    "        nc = int(nc/2)\n",
    "        return x[:, :nc] * F.sigmoid(x[:, nc:])\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, bias=False):\n",
    "    \"1x1 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                     padding=0, bias=bias)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "# Upsale the spatial size by a factor of 2\n",
    "def upBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.BatchNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block\n",
    "\n",
    "\n",
    "# Keep the spatial size\n",
    "def Block3x3_relu(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        conv3x3(in_planes, out_planes * 2),\n",
    "        nn.BatchNorm2d(out_planes * 2),\n",
    "        GLU())\n",
    "    return block\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(channel_num, channel_num * 2),\n",
    "            nn.BatchNorm2d(channel_num * 2),\n",
    "            GLU(),\n",
    "            conv3x3(channel_num, channel_num),\n",
    "            nn.BatchNorm2d(channel_num))\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "\n",
    "# ############## Text2Image Encoder-Decoder #######\n",
    "class RNN_ENCODER(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
    "                 nhidden=128, nlayers=1, bidirectional=True):\n",
    "        super(RNN_ENCODER, self).__init__()\n",
    "        self.n_steps = cfg.TEXT.WORDS_NUM\n",
    "        self.ntoken = ntoken  # size of the dictionary\n",
    "        self.ninput = ninput  # size of each embedding vector\n",
    "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
    "        self.nlayers = nlayers  # Number of recurrent layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_type = cfg.RNN_TYPE\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        # number of features in the hidden state\n",
    "        self.nhidden = nhidden // self.num_directions\n",
    "\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
    "        self.drop = nn.Dropout(self.drop_prob)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            # dropout: If non-zero, introduces a dropout layer on\n",
    "            # the outputs of each RNN layer except the last layer\n",
    "            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                               self.nlayers, batch_first=True,\n",
    "                               dropout=self.drop_prob,\n",
    "                               bidirectional=self.bidirectional)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(self.ninput, self.nhidden,\n",
    "                              self.nlayers, batch_first=True,\n",
    "                              dropout=self.drop_prob,\n",
    "                              bidirectional=self.bidirectional)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, captions, cap_lens, mask=None):\n",
    "        # input: torch.LongTensor of size batch x n_steps\n",
    "        # --> emb: batch x n_steps x ninput\n",
    "        emb = self.drop(self.encoder(captions))\n",
    "        #\n",
    "        # Returns: a PackedSequence object\n",
    "        cap_lens = cap_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)  # compress before rnn \n",
    "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
    "        # tensor containing the initial hidden state for each element in batch.\n",
    "        # #output (batch, seq_len, hidden_size * num_directions)\n",
    "        # #or a PackedSequence object:\n",
    "        # tensor containing output features (h_t) from the last layer of RNN\n",
    "        output, hidden = self.rnn(emb)\n",
    "        # PackedSequence object\n",
    "        # --> (batch, seq_len, hidden_size * num_directions)\n",
    "        output = pad_packed_sequence(output, batch_first=True)[0] # decompress after rnn\n",
    "        # output = self.drop(output)\n",
    "        # --> batch x hidden_size*num_directions x seq_len\n",
    "        words_emb = output.transpose(1, 2)\n",
    "        # --> batch x num_directions*hidden_size\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
    "        else:\n",
    "            sent_emb = hidden.transpose(0, 1).contiguous()\n",
    "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
    "        return words_emb, sent_emb\n",
    "\n",
    "\n",
    "class CNN_ENCODER(nn.Module):\n",
    "    def __init__(self, nef):\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        if cfg.TRAIN.FLAG:\n",
    "            self.nef = nef\n",
    "        else:\n",
    "            self.nef = 256  # define a uniform ranker\n",
    "\n",
    "        model = models.inception_v3()\n",
    "        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n",
    "        model.load_state_dict(model_zoo.load_url(url))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('Load pretrained model from ', url)\n",
    "        # print(model)\n",
    "\n",
    "        self.define_module(model)\n",
    "        self.init_trainable_weights()\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n",
    "        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n",
    "        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n",
    "        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n",
    "        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n",
    "        self.Mixed_5b = model.Mixed_5b\n",
    "        self.Mixed_5c = model.Mixed_5c\n",
    "        self.Mixed_5d = model.Mixed_5d\n",
    "        self.Mixed_6a = model.Mixed_6a\n",
    "        self.Mixed_6b = model.Mixed_6b\n",
    "        self.Mixed_6c = model.Mixed_6c\n",
    "        self.Mixed_6d = model.Mixed_6d\n",
    "        self.Mixed_6e = model.Mixed_6e\n",
    "        self.Mixed_7a = model.Mixed_7a\n",
    "        self.Mixed_7b = model.Mixed_7b\n",
    "        self.Mixed_7c = model.Mixed_7c\n",
    "\n",
    "        self.emb_features = conv1x1(768, self.nef)\n",
    "        self.emb_cnn_code = nn.Linear(2048, self.nef)\n",
    "\n",
    "    def init_trainable_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.emb_features.weight.data.uniform_(-initrange, initrange)\n",
    "        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = None\n",
    "        # --> fixed-size input: batch x 3 x 299 x 299\n",
    "        x = nn.Upsample(size=(299, 299), mode='bilinear')(x)\n",
    "        # 299 x 299 x 3\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # 149 x 149 x 32\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # 147 x 147 x 32\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # 147 x 147 x 64\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 73 x 73 x 64\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # 73 x 73 x 80\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # 71 x 71 x 192\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # 35 x 35 x 192\n",
    "        x = self.Mixed_5b(x)\n",
    "        # 35 x 35 x 256\n",
    "        x = self.Mixed_5c(x)\n",
    "        # 35 x 35 x 288\n",
    "        x = self.Mixed_5d(x)\n",
    "        # 35 x 35 x 288\n",
    "\n",
    "        x = self.Mixed_6a(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6b(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6c(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6d(x)\n",
    "        # 17 x 17 x 768\n",
    "        x = self.Mixed_6e(x)\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        # image region features\n",
    "        features = x\n",
    "        # 17 x 17 x 768\n",
    "\n",
    "        x = self.Mixed_7a(x)\n",
    "        # 8 x 8 x 1280\n",
    "        x = self.Mixed_7b(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = self.Mixed_7c(x)\n",
    "        # 8 x 8 x 2048\n",
    "        x = F.avg_pool2d(x, kernel_size=8)\n",
    "        # 1 x 1 x 2048\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # 1 x 1 x 2048\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 2048\n",
    "\n",
    "        # global image features\n",
    "        cnn_code = self.emb_cnn_code(x)\n",
    "        # 512\n",
    "        if features is not None:\n",
    "            features = self.emb_features(features)\n",
    "        return features, cnn_code\n",
    "\n",
    "\n",
    "# ############## G networks ###################\n",
    "class CA_NET(nn.Module):\n",
    "    # some code is modified from vae examples\n",
    "    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "    def __init__(self):\n",
    "        super(CA_NET, self).__init__()\n",
    "        self.t_dim = cfg.TEXT.EMBEDDING_DIM\n",
    "        self.c_dim = cfg.GAN.CONDITION_DIM\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True) # 4 times\n",
    "        self.relu = GLU()\n",
    "\n",
    "    def encode(self, text_embedding):\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        \n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if cfg.CUDA:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = eps\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, text_embedding):\n",
    "        mu, logvar = self.encode(text_embedding) # mu is the main thing that we will use in the conditional dim\n",
    "        c_code = self.reparametrize(mu, logvar) # don't change mu, but the c_code is set by the logvar, mu + noise = c_code        return c_code, mu, logvar\n",
    "        return c_code, mu, logvar\n",
    "\n",
    "class INIT_STAGE_G(nn.Module):\n",
    "    def __init__(self, ngf, ncf):\n",
    "        super(INIT_STAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.in_dim = cfg.GAN.Z_DIM + ncf  # cfg.TEXT.EMBEDDING_DIM\n",
    "\n",
    "        self.define_module()\n",
    "\n",
    "    def define_module(self):\n",
    "        nz, ngf = self.in_dim, self.gf_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n",
    "            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n",
    "            GLU())\n",
    "\n",
    "        self.upsample1 = upBlock(ngf, ngf // 2)\n",
    "        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n",
    "        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n",
    "        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n",
    "\n",
    "    def forward(self, z_code, c_code):\n",
    "        \"\"\"\n",
    "        :param z_code: batch x cfg.GAN.Z_DIM\n",
    "        :param c_code: batch x cfg.TEXT.EMBEDDING_DIM\n",
    "        :return: batch x ngf/16 x 64 x 64\n",
    "        \"\"\"\n",
    "        c_z_code = torch.cat((c_code, z_code), 1)\n",
    "        # state size ngf x 4 x 4\n",
    "        out_code = self.fc(c_z_code)\n",
    "        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n",
    "        # state size ngf/3 x 8 x 8\n",
    "        out_code = self.upsample1(out_code)\n",
    "        # state size ngf/4 x 16 x 16\n",
    "        out_code = self.upsample2(out_code)\n",
    "        # state size ngf/8 x 32 x 32\n",
    "        out_code32 = self.upsample3(out_code)\n",
    "        # state size ngf/16 x 64 x 64\n",
    "        out_code64 = self.upsample4(out_code32)\n",
    "\n",
    "        return out_code64\n",
    "\n",
    "\n",
    "class NEXT_STAGE_G(nn.Module):\n",
    "    def __init__(self, ngf, nef, ncf):\n",
    "        super(NEXT_STAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.ef_dim = nef\n",
    "        self.cf_dim = ncf\n",
    "        self.num_residual = cfg.GAN.R_NUM\n",
    "        self.define_module()\n",
    "\n",
    "    def _make_layer(self, block, channel_num):\n",
    "        layers = []\n",
    "        for i in range(cfg.GAN.R_NUM):\n",
    "            layers.append(block(channel_num))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def define_module(self):\n",
    "        ngf = self.gf_dim\n",
    "        self.att = GlobalAttentionGeneral(ngf, self.ef_dim)  \n",
    "        self.residual = self._make_layer(ResBlock, ngf * 2) \n",
    "        self.upsample = upBlock(ngf * 2, ngf)\n",
    "\n",
    "    def forward(self, h_code, c_code, word_embs, mask):\n",
    "        \"\"\"\n",
    "            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n",
    "            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n",
    "            c_code1: batch x idf x queryL\n",
    "            att1: batch x sourceL x queryL\n",
    "        \"\"\"\n",
    "        self.att.applyMask(mask)\n",
    "        c_code, att = self.att(h_code, word_embs) # input, context\n",
    "        h_c_code = torch.cat((h_code, c_code), 1)\n",
    "        out_code = self.residual(h_c_code)\n",
    "\n",
    "        # state size ngf/2 x 2in_size x 2in_size\n",
    "        out_code = self.upsample(out_code)\n",
    "\n",
    "        return out_code, att\n",
    "\n",
    "\n",
    "class GET_IMAGE_G(nn.Module):\n",
    "    def __init__(self, ngf):\n",
    "        super(GET_IMAGE_G, self).__init__()\n",
    "        self.gf_dim = ngf\n",
    "        self.img = nn.Sequential(\n",
    "            conv3x3(ngf, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, h_code):\n",
    "        out_img = self.img(h_code)\n",
    "        return out_img\n",
    "\n",
    "\n",
    "class G_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_NET, self).__init__()\n",
    "        ngf = cfg.GAN.GF_DIM\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM\n",
    "        ncf = cfg.GAN.CONDITION_DIM\n",
    "        self.ca_net = CA_NET()\n",
    "\n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
    "            self.img_net1 = GET_IMAGE_G(ngf)\n",
    "        # gf x 64 x 64\n",
    "        if cfg.TREE.BRANCH_NUM > 1:\n",
    "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
    "            self.img_net2 = GET_IMAGE_G(ngf)\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
    "            self.img_net3 = GET_IMAGE_G(ngf)\n",
    "\n",
    "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
    "        \"\"\"\n",
    "            :param z_code: batch x cfg.GAN.Z_DIM\n",
    "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
    "            :param word_embs: batch x cdf x seq_len\n",
    "            :param mask: batch x seq_len\n",
    "            :return:\n",
    "        \"\"\"\n",
    "        fake_imgs = []\n",
    "        att_maps = []\n",
    "        \n",
    "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
    "        \n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            h_code1 = self.h_net1(z_code, c_code)\n",
    "            fake_img1 = self.img_net1(h_code1)\n",
    "            fake_imgs.append(fake_img1)\n",
    "        if cfg.TREE.BRANCH_NUM > 1:\n",
    "            h_code2, att1 = \\\n",
    "                self.h_net2(h_code1, c_code, word_embs, mask)\n",
    "            fake_img2 = self.img_net2(h_code2)\n",
    "            fake_imgs.append(fake_img2)\n",
    "            if att1 is not None:\n",
    "                att_maps.append(att1)\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            h_code3, att2 = \\\n",
    "                self.h_net3(h_code2, c_code, word_embs, mask)\n",
    "            fake_img3 = self.img_net3(h_code3)\n",
    "            fake_imgs.append(fake_img3)\n",
    "            if att2 is not None:\n",
    "                att_maps.append(att2)\n",
    "\n",
    "        return fake_imgs, att_maps, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "class G_DCGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_DCGAN, self).__init__()\n",
    "        ngf = cfg.GAN.GF_DIM\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM\n",
    "        ncf = cfg.GAN.CONDITION_DIM\n",
    "        self.ca_net = CA_NET()\n",
    "\n",
    "        # 16gf x 64 x 64 --> gf x 64 x 64 --> 3 x 64 x 64\n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n",
    "        # gf x 64 x 64\n",
    "        if cfg.TREE.BRANCH_NUM > 1:\n",
    "            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n",
    "        self.img_net = GET_IMAGE_G(ngf)\n",
    "\n",
    "    def forward(self, z_code, sent_emb, word_embs, mask):\n",
    "        \"\"\"\n",
    "            :param z_code: batch x cfg.GAN.Z_DIM\n",
    "            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n",
    "            :param word_embs: batch x cdf x seq_len\n",
    "            :param mask: batch x seq_len\n",
    "            :return:\n",
    "        \"\"\"\n",
    "        att_maps = []\n",
    "        c_code, mu, logvar = self.ca_net(sent_emb)\n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            h_code = self.h_net1(z_code, c_code)\n",
    "        if cfg.TREE.BRANCH_NUM > 1:\n",
    "            h_code, att1 = self.h_net2(h_code, c_code, word_embs, mask)\n",
    "            if att1 is not None:\n",
    "                att_maps.append(att1)\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            h_code, att2 = self.h_net3(h_code, c_code, word_embs, mask)\n",
    "            if att2 is not None:\n",
    "                att_maps.append(att2)\n",
    "\n",
    "        fake_imgs = self.img_net(h_code)\n",
    "        return [fake_imgs], att_maps, mu, logvar\n",
    "\n",
    "\n",
    "# ############## D networks ##########################\n",
    "def Block3x3_leakRelu(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        conv3x3(in_planes, out_planes),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "# Downsale the spatial size by a factor of 2\n",
    "def downBlock(in_planes, out_planes):\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_planes),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "# Downsale the spatial size by a factor of 16\n",
    "def encode_image_by_16times(ndf):\n",
    "    encode_img = nn.Sequential(\n",
    "        # --> state size. ndf x in_size/2 x in_size/2\n",
    "        nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # --> state size 2ndf x x in_size/4 x in_size/4\n",
    "        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 2),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # --> state size 4ndf x in_size/8 x in_size/8\n",
    "        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 4),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # --> state size 8ndf x in_size/16 x in_size/16\n",
    "        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(ndf * 8),\n",
    "        nn.LeakyReLU(0.2, inplace=True)\n",
    "    )\n",
    "    return encode_img\n",
    "\n",
    "\n",
    "class D_GET_LOGITS(nn.Module):\n",
    "    def __init__(self, ndf, nef, bcondition=False):\n",
    "        super(D_GET_LOGITS, self).__init__()\n",
    "        self.df_dim = ndf\n",
    "        self.ef_dim = nef\n",
    "        self.bcondition = bcondition\n",
    "        if self.bcondition:\n",
    "            self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n",
    "\n",
    "        self.outlogits = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, h_code, c_code=None):\n",
    "        if self.bcondition and c_code is not None:\n",
    "            # conditioning output\n",
    "            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n",
    "            c_code = c_code.repeat(1, 1, 4, 4)\n",
    "            # state size (ngf+egf) x 4 x 4\n",
    "            h_c_code = torch.cat((h_code, c_code), 1)\n",
    "            # state size ngf x in_size x in_size\n",
    "            h_c_code = self.jointConv(h_c_code)\n",
    "        else:\n",
    "            h_c_code = h_code\n",
    "\n",
    "        output = self.outlogits(h_c_code)\n",
    "        return output.view(-1)\n",
    "\n",
    "\n",
    "# For 64 x 64 images\n",
    "class D_NET64(nn.Module):\n",
    "    def __init__(self, b_jcu=True):\n",
    "        super(D_NET64, self).__init__()\n",
    "        ndf = cfg.GAN.DF_DIM\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM\n",
    "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
    "        if b_jcu:\n",
    "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "        else:\n",
    "            self.UNCOND_DNET = None\n",
    "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "\n",
    "    def forward(self, x_var):\n",
    "        x_code4 = self.img_code_s16(x_var)  # 4 x 4 x 8df\n",
    "        return x_code4\n",
    "\n",
    "\n",
    "# For 128 x 128 images\n",
    "class D_NET128(nn.Module):\n",
    "    def __init__(self, b_jcu=True):\n",
    "        super(D_NET128, self).__init__()\n",
    "        ndf = cfg.GAN.DF_DIM\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM\n",
    "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
    "        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n",
    "        self.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n",
    "        #\n",
    "        if b_jcu:\n",
    "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "        else:\n",
    "            self.UNCOND_DNET = None\n",
    "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "\n",
    "    def forward(self, x_var):\n",
    "        x_code8 = self.img_code_s16(x_var)   # 8 x 8 x 8df\n",
    "        x_code4 = self.img_code_s32(x_code8)   # 4 x 4 x 16df\n",
    "        x_code4 = self.img_code_s32_1(x_code4)  # 4 x 4 x 8df\n",
    "        return x_code4\n",
    "\n",
    "\n",
    "# For 256 x 256 images\n",
    "class D_NET256(nn.Module):\n",
    "    def __init__(self, b_jcu=True):\n",
    "        super(D_NET256, self).__init__()\n",
    "        ndf = cfg.GAN.DF_DIM\n",
    "        nef = cfg.TEXT.EMBEDDING_DIM\n",
    "        self.img_code_s16 = encode_image_by_16times(ndf)\n",
    "        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n",
    "        self.img_code_s64 = downBlock(ndf * 16, ndf * 32)\n",
    "        self.img_code_s64_1 = Block3x3_leakRelu(ndf * 32, ndf * 16)\n",
    "        self.img_code_s64_2 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n",
    "        if b_jcu:\n",
    "            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n",
    "        else:\n",
    "            self.UNCOND_DNET = None\n",
    "        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n",
    "\n",
    "    def forward(self, x_var):\n",
    "        x_code16 = self.img_code_s16(x_var)\n",
    "        x_code8 = self.img_code_s32(x_code16)\n",
    "        x_code4 = self.img_code_s64(x_code8)\n",
    "        x_code4 = self.img_code_s64_1(x_code4)\n",
    "        x_code4 = self.img_code_s64_2(x_code4)\n",
    "        return x_code4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"Returns cosine similarity between x1 and x2, computed along dim.\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.orthogonal_(m.weight.data, 1.0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.orthogonal_(m.weight.data, 1.0)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.0)\n",
    "            \n",
    "def load_params(model, new_param):\n",
    "    for p, new_p in zip(model.parameters(), new_param):\n",
    "        p.data.copy_(new_p)   \n",
    "\n",
    "\n",
    "def copy_G_params(model):\n",
    "    flatten = deepcopy(list(p.data for p in model.parameters())) # won't change \n",
    "    return flatten\n",
    "\n",
    "        \n",
    "        \n",
    "class AttnGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttnGAN, self).__init__()\n",
    "        if not os.path.exists(os.path.join(cfg.MODEL_PATH)):\n",
    "            os.makedirs(os.path.join(cfg.MODEL_PATH, 'Model'))\n",
    "        if not os.path.exists('Image'):\n",
    "            os.makedirs('Image')\n",
    "            \n",
    "        cudnn.benchmark = True\n",
    "        self.model_dir = cfg.MODEL_PATH\n",
    "        self.batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n",
    "        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n",
    "        self.build_models()\n",
    "        self.define_optimizers()\n",
    "        self.prepare_labels()\n",
    "        self.avg_param_G = self.copy_G_params()\n",
    "\n",
    "        \n",
    "    def build_models(self):\n",
    "        if cfg.TRAIN.NET_E == '':\n",
    "                print('Error: no pretrained text-image encoders')\n",
    "                return\n",
    "\n",
    "        ########################## Image Encoder ##########################    \n",
    "\n",
    "        self.image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "        img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n",
    "        state_dict = \\\n",
    "            torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n",
    "        self.image_encoder.load_state_dict(state_dict)\n",
    "        for p in self.image_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print('Load image encoder from:', img_encoder_path)\n",
    "        self.image_encoder.eval()\n",
    "\n",
    "        ########################## Text Encoder ##########################\n",
    "        self.text_encoder = \\\n",
    "            RNN_ENCODER(cfg.N_WORDS, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "        state_dict = \\\n",
    "            torch.load(cfg.TRAIN.NET_E,\n",
    "                       map_location=lambda storage, loc: storage)\n",
    "        self.text_encoder.load_state_dict(state_dict)\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        print('Load text encoder from:', cfg.TRAIN.NET_E)\n",
    "        self.text_encoder.eval()\n",
    "\n",
    "        ########################## G & D ##########################\n",
    "\n",
    "        self.netsD = []\n",
    "        self.netG = G_NET()\n",
    "        if cfg.TREE.BRANCH_NUM > 0:\n",
    "            self.netsD.append(D_NET64())\n",
    "        if cfg.TREE.BRANCH_NUM > 1:\n",
    "            self.netsD.append(D_NET128())\n",
    "        if cfg.TREE.BRANCH_NUM > 2:\n",
    "            self.netsD.append(D_NET256())\n",
    "        self.netG.apply(weights_init)\n",
    "        for i in range(len(self.netsD)):\n",
    "            self.netsD[i].apply(weights_init)\n",
    "        print('# of netsD', len(self.netsD))\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        self.optimizersD = []\n",
    "        num_Ds = len(self.netsD)\n",
    "        for i in range(num_Ds):\n",
    "            opt = optim.Adam(self.netsD[i].parameters(),\n",
    "                             lr=cfg.TRAIN.DISCRIMINATOR_LR,\n",
    "                             betas=(0.5, 0.999))\n",
    "            self.optimizersD.append(opt)\n",
    "\n",
    "        self.optimizerG = optim.Adam(self.netG.parameters(),\n",
    "                                lr=cfg.TRAIN.GENERATOR_LR,\n",
    "                                betas=(0.5, 0.999))\n",
    "\n",
    "    def prepare_labels(self):\n",
    "        batch_size = self.batch_size\n",
    "        self.real_labels = torch.FloatTensor(batch_size).fill_(1)\n",
    "        self.fake_labels = torch.FloatTensor(batch_size).fill_(0)\n",
    "        self.match_labels = torch.LongTensor(range(batch_size))\n",
    "\n",
    "\n",
    "    def save_model(self, steps):\n",
    "\n",
    "        backup_para = copy_G_params(self.netG)\n",
    "        load_params(self.netG, self.avg_param_G)\n",
    "        torch.save(self.netG.state_dict(),\n",
    "            '%s/netG_epoch_%d.pth' % (self.model_dir, steps))\n",
    "        load_params(self.netG, backup_para)\n",
    "        #\n",
    "        for i in range(len(self.netsD)):\n",
    "            netD = self.netsD[i]\n",
    "            torch.save(netD.state_dict(),\n",
    "                '%s/netD%d.pth' % (self.model_dir, i))\n",
    "        print('Save G/Ds models.')\n",
    "\n",
    "    def set_requires_grad_value(self, models_list, brequires):\n",
    "        for i in range(len(models_list)):\n",
    "            for p in models_list[i].parameters():\n",
    "                p.requires_grad = brequires\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "\n",
    "\n",
    "        nz = cfg.GAN.Z_DIM\n",
    "        noise = torch.FloatTensor(self.batch_size, nz)\n",
    "        fixed_noise = torch.FloatTensor(self.batch_size, nz).normal_(0, 1)\n",
    "\n",
    "        self.imgs, self.captions, self.cap_lens, self.class_ids, self.keys = prepare_data(data)\n",
    "\n",
    "\n",
    "        words_embs, sent_emb = self.text_encoder(self.captions, self.cap_lens)\n",
    "        words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
    "        mask = (self.captions == 0) # shorter sentence will not be punished by the 0 padding\n",
    "        num_words = words_embs.size(2)\n",
    "        if mask.size(1) > num_words:\n",
    "            mask = mask[:, :num_words]\n",
    "\n",
    "        ####################### Generate fake image #######################\n",
    "\n",
    "\n",
    "        noise = torch.FloatTensor(self.batch_size, nz).normal_(0, 1)\n",
    "        self.fake_imgs, _, mu, logvar = self.netG(noise, sent_emb, words_embs, mask)\n",
    "\n",
    "        ####################### D Loss #######################\n",
    "        self.errD_total = 0\n",
    "        D_logs = ''\n",
    "        for i in range(len(self.netsD)):\n",
    "            self.netsD[i].zero_grad()\n",
    "\n",
    "            ######## Get scores ########\n",
    "            real_features = self.netsD[i](self.imgs[i])\n",
    "            fake_features = self.netsD[i](self.fake_imgs[i].detach())\n",
    "\n",
    "            ######## Get Loss ########\n",
    "            errD = self.discriminator_loss(real_features, fake_features, sent_emb, self.netsD[i])\n",
    "\n",
    "            ######## Backward ########\n",
    "            errD.backward()\n",
    "            self.optimizersD[i].step()\n",
    "            self.errD_total += errD\n",
    "            D_logs += 'errD%d: %.2f ' % (i, errD.data[0])\n",
    "\n",
    "\n",
    "\n",
    "        ####################### G Loss #######################   \n",
    "\n",
    "        self.netG.zero_grad()\n",
    "\n",
    "        ########## Normal G Loss ##########\n",
    "        self.g_loss = self.generator_loss(sent_emb)\n",
    "\n",
    "        ########## Word Loss ##########\n",
    "        region_features, cnn_code = self.image_encoder(self.fake_imgs[-1])\n",
    "        w_loss0, w_loss1, _ = self.words_loss(region_features, words_embs)                                           \n",
    "        self.w_loss = (w_loss0 + w_loss1) * cfg.TRAIN.SMOOTH.LAMBDA\n",
    "\n",
    "        ########## Sentence Loss ##########\n",
    "        s_loss0, s_loss1 = self.sent_loss(cnn_code, sent_emb)  # sent_emb and cnn_code has to be similar\n",
    "        self.s_loss = (s_loss0 + s_loss1) * cfg.TRAIN.SMOOTH.LAMBDA\n",
    "\n",
    "        ########## KL Loss ##########\n",
    "        self.kl_loss = self.KL_loss(mu, logvar)\n",
    "\n",
    "        self.errG_total = self.w_loss + self.s_loss + self.kl_loss + self.g_loss\n",
    "\n",
    "        self.errG_total.backward()\n",
    "        self.optimizerG.step()\n",
    "\n",
    "        ####################### Get Average G params #######################  \n",
    "\n",
    "        for p, avg_p in zip(self.netG.parameters(), self.avg_param_G):\n",
    "            avg_p.mul_(0.999).add_(0.001, p.data)\n",
    "\n",
    "\n",
    "    def generator_loss(self, sent_emb):\n",
    "\n",
    "        errG_total = 0 \n",
    "        for i in range(len(self.netsD)):\n",
    "            features = self.netsD[i](self.fake_imgs[i])\n",
    "            cond_logits = self.netsD[i].COND_DNET(features, sent_emb)\n",
    "            cond_errG = nn.BCELoss()(cond_logits, self.real_labels)\n",
    "            if self.netsD[i].UNCOND_DNET is  not None:\n",
    "                logits = self.netsD[i].UNCOND_DNET(features)\n",
    "                errG = nn.BCELoss()(logits, self.real_labels)\n",
    "                g_loss = errG + cond_errG\n",
    "            else:\n",
    "                g_loss = cond_errG\n",
    "            errG_total += g_loss\n",
    "\n",
    "        return errG_total\n",
    "\n",
    "    def KL_loss(self,mu, logvar):\n",
    "        # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "        KLD = torch.mean(KLD_element).mul_(-0.5)\n",
    "        return KLD\n",
    "\n",
    "\n",
    "\n",
    "    def sent_loss(self,cnn_code, rnn_code, eps=1e-8):\n",
    "        # ### Mask mis-match samples  ###\n",
    "        # that come from the same class as the real sample ###\n",
    "\n",
    "        labels = self.match_labels\n",
    "        class_ids = self.class_ids\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        masks = []\n",
    "        if class_ids is not None:\n",
    "            for i in range(batch_size):\n",
    "                mask = (class_ids == class_ids[i]).astype(np.uint8)\n",
    "                mask[i] = 0\n",
    "                masks.append(mask.reshape((1, -1)))\n",
    "            masks = np.concatenate(masks, 0)\n",
    "            # masks: batch_size x batch_size\n",
    "            masks = torch.ByteTensor(masks)\n",
    "\n",
    "        # --> seq_len x batch_size x nef\n",
    "        if cnn_code.dim() == 2:\n",
    "            cnn_code = cnn_code.unsqueeze(0)\n",
    "            rnn_code = rnn_code.unsqueeze(0)\n",
    "\n",
    "        # cnn_code_norm / rnn_code_norm: seq_len x batch_size x 1\n",
    "        cnn_code_norm = torch.norm(cnn_code, 2, dim=2, keepdim=True)\n",
    "        rnn_code_norm = torch.norm(rnn_code, 2, dim=2, keepdim=True)\n",
    "        # scores* / norm*: seq_len x batch_size x batch_size\n",
    "        scores0 = torch.bmm(cnn_code, rnn_code.transpose(1, 2))\n",
    "        norm0 = torch.bmm(cnn_code_norm, rnn_code_norm.transpose(1, 2))\n",
    "        scores0 = scores0 / norm0.clamp(min=eps) * cfg.TRAIN.SMOOTH.GAMMA3\n",
    "\n",
    "        # --> batch_size x batch_size\n",
    "        scores0 = scores0.squeeze()\n",
    "        if class_ids is not None:\n",
    "            scores0.data.masked_fill_(masks, -float('inf')) # where mask == 1, it will fill the socores by -inf\n",
    "        scores1 = scores0.transpose(0, 1)\n",
    "        if labels is not None:\n",
    "            loss0 = nn.CrossEntropyLoss()(scores0, labels)\n",
    "            loss1 = nn.CrossEntropyLoss()(scores1, labels)\n",
    "        else:\n",
    "            loss0, loss1 = None, None\n",
    "        return loss0, loss1\n",
    "\n",
    "\n",
    "    def words_loss(self, img_features, words_emb):\n",
    "        \"\"\"\n",
    "            words_emb(query): batch x nef x seq_len\n",
    "            img_features(context): batch x nef x 17 x 17\n",
    "        \"\"\"\n",
    "        labels = self.match_labels\n",
    "        batch_size = self.batch_size\n",
    "        class_ids = self.class_ids\n",
    "        cap_lens = self.cap_lens\n",
    "        masks = []\n",
    "        att_maps = []\n",
    "        similarities = []\n",
    "        cap_lens = self.cap_lens.data.tolist()\n",
    "        for i in range(self.batch_size):\n",
    "            if self.class_ids is not None:\n",
    "                mask = (self.class_ids == self.class_ids[i]).astype(np.uint8)\n",
    "                mask[i] = 0\n",
    "                masks.append(mask.reshape((1, -1)))\n",
    "            # Get the i-th text description\n",
    "            words_num = cap_lens[i]\n",
    "            # -> 1 x nef x words_num\n",
    "            word = words_emb[i, :, :words_num].unsqueeze(0).contiguous()\n",
    "            # -> batch_size x nef x words_num\n",
    "            word = word.repeat(batch_size, 1, 1)\n",
    "            # batch x nef x 17*17\n",
    "            context = img_features\n",
    "            \"\"\"\n",
    "                word(query): batch x nef x words_num\n",
    "                context: batch x nef x 17 x 17\n",
    "                weiContext: batch x nef x words_num\n",
    "                attn: batch x words_num x 17 x 17\n",
    "            \"\"\"\n",
    "            weiContext, attn = func_attention(word, context, cfg.TRAIN.SMOOTH.GAMMA1)\n",
    "            att_maps.append(attn[i].unsqueeze(0).contiguous())\n",
    "            # --> batch_size x words_num x nef\n",
    "            word = word.transpose(1, 2).contiguous()\n",
    "            weiContext = weiContext.transpose(1, 2).contiguous()\n",
    "            # --> batch_size*words_num x nef\n",
    "            word = word.view(batch_size * words_num, -1)\n",
    "            weiContext = weiContext.view(batch_size * words_num, -1)\n",
    "            #\n",
    "            # -->batch_size*words_num\n",
    "            row_sim = cosine_similarity(word, weiContext)\n",
    "            # --> batch_size x words_num\n",
    "            row_sim = row_sim.view(batch_size, words_num)\n",
    "\n",
    "            # Eq. (10)\n",
    "            row_sim.mul_(cfg.TRAIN.SMOOTH.GAMMA2).exp_()\n",
    "            row_sim = row_sim.sum(dim=1, keepdim=True)\n",
    "            row_sim = torch.log(row_sim)\n",
    "\n",
    "            # --> 1 x batch_size\n",
    "            # similarities(i, j): the similarity between the i-th image and the j-th text description\n",
    "            similarities.append(row_sim)\n",
    "\n",
    "        # batch_size x batch_size\n",
    "        similarities = torch.cat(similarities, 1)\n",
    "        if class_ids is not None:\n",
    "            masks = np.concatenate(masks, 0)\n",
    "            # masks: batch_size x batch_size\n",
    "            masks = torch.ByteTensor(masks)\n",
    "\n",
    "        similarities = similarities * cfg.TRAIN.SMOOTH.GAMMA3\n",
    "        if class_ids is not None:\n",
    "            similarities.data.masked_fill_(masks, -float('inf'))\n",
    "        similarities1 = similarities.transpose(0, 1)\n",
    "        if labels is not None:\n",
    "            loss0 = nn.CrossEntropyLoss()(similarities, labels)\n",
    "            loss1 = nn.CrossEntropyLoss()(similarities1, labels)\n",
    "        else:\n",
    "            loss0, loss1 = None, None\n",
    "        return loss0, loss1, att_maps\n",
    "\n",
    "\n",
    "\n",
    "    def discriminator_loss(self, real_features, fake_features, sent_emb, netsD):\n",
    "\n",
    "        cond_real_logits = netsD.COND_DNET(real_features, sent_emb)\n",
    "        cond_real_errD = nn.BCELoss()(cond_real_logits, self.real_labels)\n",
    "\n",
    "        cond_fake_logits = netsD.COND_DNET(fake_features, sent_emb)\n",
    "        cond_fake_errD = nn.BCELoss()(cond_fake_logits, self.fake_labels)\n",
    "\n",
    "        cond_wrong_logits = netsD.COND_DNET(real_features[:(self.batch_size - 1)], sent_emb[1:self.batch_size])\n",
    "        cond_wrong_errD = nn.BCELoss()(cond_wrong_logits, self.fake_labels[1:self.batch_size])\n",
    "        if netsD.UNCOND_DNET is not None:\n",
    "            real_logits = netsD.UNCOND_DNET(real_features)\n",
    "            fake_logits = netsD.UNCOND_DNET(fake_features)\n",
    "\n",
    "            real_errD = nn.BCELoss()(real_logits, self.real_labels)\n",
    "            fake_errD = nn.BCELoss()(fake_logits, self.fake_labels)\n",
    "\n",
    "            errD = ((real_errD + cond_real_errD) / 2. +\n",
    "                    (fake_errD + cond_fake_errD + cond_wrong_errD) / 3.)\n",
    "        else:\n",
    "            errD = cond_real_errD + (cond_fake_errD + cond_wrong_errD) / 2.\n",
    "        return errD\n",
    "\n",
    "    def copy_G_params(self):\n",
    "        self.backup = deepcopy(list(p.data for p in self.netG.parameters()))\n",
    "        return self.backup\n",
    "\n",
    "\n",
    "    def load_params(self, new_param):\n",
    "        # Loading params for G\n",
    "        for p, new_p in zip(self.netG.parameters(), new_param):\n",
    "            p.data.copy_(new_p)    \n",
    "\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from: birds_everything.pickle\n",
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
      "Load image encoder from: ../DAMSMencoders/bird/image_encoder200.pth\n",
      "Load text encoder from: ../DAMSMencoders/bird/text_encoder200.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of netsD 3\n"
     ]
    }
   ],
   "source": [
    "##### Prepare for training #####\n",
    "imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM - 1))\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.Resize(int(imsize * 76 / 64)),\n",
    "        transforms.RandomCrop(imsize),\n",
    "        transforms.RandomHorizontalFlip()])\n",
    "split_dir, bshuffle = 'train', True\n",
    "\n",
    "dataset = TextDataset(cfg.DATA_DIR, split_dir,\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.BATCH_SIZE,\n",
    "                                         drop_last=True, shuffle=bshuffle, num_workers=int(cfg.WORKERS))\n",
    "cfg.N_WORDS = dataset.n_words\n",
    "\n",
    "attnGAN = AttnGAN()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:173: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1749: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [0] | batch [0] | D Loss: [4.5398] | G Loss: [73.2060] | Time: 85.7s\n",
      "| G Loss [70.2684] | W Loss : [1.3507] | S Loss: [1.5803] | KL Loss: [0.0066] |\n",
      "Save G/Ds models.\n",
      "Model saved\n",
      "| Epoch [0] | batch [1] | D Loss: [6.9980] | G Loss: [48.5986] | Time: 85.1s\n",
      "| G Loss [44.3884] | W Loss : [1.4704] | S Loss: [2.7314] | KL Loss: [0.0084] |\n",
      "Save G/Ds models.\n",
      "Model saved\n",
      "| Epoch [0] | batch [2] | D Loss: [4.5419] | G Loss: [60.8119] | Time: 78.0s\n",
      "| G Loss [57.8802] | W Loss : [1.3462] | S Loss: [1.5709] | KL Loss: [0.0145] |\n",
      "Save G/Ds models.\n",
      "Model saved\n",
      "| Epoch [0] | batch [3] | D Loss: [7.9174] | G Loss: [67.7476] | Time: 87.5s\n",
      "| G Loss [63.5478] | W Loss : [1.8555] | S Loss: [2.3317] | KL Loss: [0.0126] |\n",
      "Save G/Ds models.\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "avg_param_G = attnGAN.copy_G_params()\n",
    "all_steps = 1\n",
    "epoch = 0\n",
    "while epoch < cfg.TRAIN.MAX_EPOCH:\n",
    "    for i,data in enumerate(dataloader):\n",
    "        start_t = time.time()\n",
    "        attnGAN(data)\n",
    "        end_t = time.time()\n",
    "        print('| Epoch [%d] | batch [%d] | D Loss: [%.4f] | G Loss: [%.4f] | Time: %.1fs' %\\\n",
    "              (epoch, i, attnGAN.errD_total.item(), attnGAN.errG_total.item(), end_t - start_t))\n",
    "        \n",
    "    \n",
    "        if all_steps % 20 == 0:  \n",
    "            print('| G Loss [%.4f] | W Loss : [%.4f] | S Loss: [%.4f] | KL Loss: [%.4f] |' %\\\n",
    "              (attnGAN.g_loss.item() , attnGAN.w_loss.item(), attnGAN.s_loss.item(), attnGAN.kl_loss.item()))\n",
    "            \n",
    "        if all_steps % 500 == 0:\n",
    "            attnGAN.save_model(all_steps)\n",
    "            print('Model saved')\n",
    "                \n",
    "        all_steps += 1    \n",
    "        \n",
    "        if i > 2:\n",
    "            break\n",
    "           \n",
    "    epoch += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
